# Comment distribuer une base de donn√©es Postgres en utilisant CitusData

*Citus transforme Postgres en une base de donn√©es distribu√©e, pour offrir √† votre application des performances √©lev√©es, quelle que soit l'√©chelle.*

# Auteurs ‚úç

Olivier GOMES

Vincent HARDOUIN

Hugo PAUTHIER

# Introduction üëã

Vous utilisez Postgres et vous chercher un moyen de scale votre base de donn√©es ? CitusData peut √™tre la solution. Cet outil est construit comme une extension c‚Äôest √† dire sur la base du projet Postgres. Cela permet ainsi de garder la quasi totalit√© de fonctionnalit√©s de Postgres tout en garantissant le maximum de compatibilit√©.

Les avantages de Citus sont mutliples :

- Scaling horizontal
- Open Source
- Meilleure performance des requ√™tes

CitusData sera favoris√© dans diff√©rents cas :

- Besoin de performances : Votre Postgres ne poss√®de qu‚Äôun seul n≈ìud et n'est donc pas assez performant et ne peut pas suivre les pics de votre charge de travail.
- Besoin d‚Äô√©volution : Votre application se d√©veloppe rapidement et vous voulez pr√©parer votre d√©ploiement Postgres avant de rencontrer des probl√®mes de performances.

En revanche, si Citus √©tend Postgres avec des fonctionnalit√©s distribu√©es, il ne va pas pour autant √™tre un remplacement imm√©diat permettant de scale toutes les charges de travail. Un cluster Citus performant implique une r√©flexion sur le mod√®le de donn√©es, l'outillage et/ou le choix des fonctionnalit√©s SQL utilis√©es. Il faut consid√©rer aussi que certaines charges de travail n'ont pas besoin d'une puissante base de donn√©es distribu√©e, bien que d'autres exigent des flux importants d'informations entre les diff√©rentes n≈ìuds. Dans ces deux cas, Citus ne sera pas le plus optimis√© et n‚Äôest donc pas recommand√©.

A la suite de cette introduction nous expliquerons comment op√©rer une migration d‚Äôun Postgres monolithique vers un Postgres ditribu√©.

# M√©thodologie üìë

Nous utiliserons le sch√©ma suivant :

```sql
marques (#marque_id, nom, chiffre_affaire, siege_social) 
voitures (#voiture_id, modele, couleur, marque_id) 
commandes (#commande_id, voiture_id)
```

![Schema_norma.png](images/schema_norma.png)

## Choix la colonne de distribution üß©

Tout d‚Äôabord, pour r√©partir de mani√®re optimale les donn√©es sur les diff√©rents *nodes*, l‚Äô√©tape primordiale est de trouver pour chaque table, la colonne √† utiliser pour la distribution. Toutes les donn√©es relatives √† cette colonne seront alors regroup√©es sur un m√™me *node.*

## D√©normalisation des tables üèì

Afin de pouvoir distribuer les donn√©es, il est n√©cessaire que la colonne de distribution soit pr√©sente dans chaque table. Ainsi, si votre base de donn√©es est normalis√©e, il convient de la d√©normaliser. Cette √©tape consiste √† rapatrier dans la table de base les colonnes qui √©taient trouvable par double jointure. Autrement dit √† ajouter la colonne de distribution dans les tables o√π elle n‚Äôy est pas. 

Actuellement pour retrouver le mod√®le de la marque de la voiture depuis la commande n**¬∞**1, nous pouvions faire une jointure comme ceci :

```sql
SELECT M.nom FROM commandes C
INNER JOIN voitures V ON C.voiture_id = V.voiture_id 
INNER JOIN marques M ON V.marque_id = M.marque_id
WHERE C.commande_id = 1;
```

`marque_id` peut √™tre une colonne de distribution, car nous en avons besoin tout le temps. 

Alors nous devons l‚Äôajouter dans la table `commandes`. Il faut √©galement ajouter la cl√© √©trang√®re `marque_id` dans la table commandes. 

```sql
BEGIN;

-- Ajoute la colonne manquante dans la table commande 
ALTER TABLE commandes ADD COLUMN marque_id uuid;

-- Remplissage de la nouvelle colonne 

UPDATE commandes 
SET marque_id = voitures.marque_id
FROM commandes 
INNER JOIN voitures 
WHERE commandes.voiture_id = voitures.voiture_id;

-- Supprime toutes les cl√©s primaire

ALTER TABLE voitures  DROP CONSTRAINT voitures_pkey CASCADE;
ALTER TABLE commandes DROP CONSTRAINT commandes_pkey CASCADE;

-- Nous recr√©ons les cl√©s primaire pour inclure la cl√© √©trang√®re : marque_id

ALTER TABLE voitures  ADD PRIMARY KEY (voiture_id, marque_id);
ALTER TABLE commandes ADD PRIMARY KEY (commande_id, marque_id);

-- Nous r√©cr√©ons les cl√©s √©trang√®res en incluant la nouvelle colonne.

ALTER TABLE commandes ADD CONSTRAINT commandes_marques_fkey
FOREIGN KEY (marque_id) REFERENCES marques (marque_id);

ALTER TABLE commandes ADD CONSTRAINT commandes_voitures_fkey
FOREIGN KEY (voiture_id, marque_id) REFERENCES voitures (voiture_id, marque_id);

COMMIT;
```

![schema_denorma.png](images/schema_denorma.png)

Ici, le remplissage de la colonne peut-√™tre tr√®s couteux. 

Nous pourrions r√©fl√©chir √† s√©parer l‚Äô√©tape de la cr√©ation de la colonne et la mise en place de cl√© primaire. Cela nous permettrait alors d‚Äôeffectuer par batch le remplissage de la nouvelle colonne : `marque_id` dans la table `commandes`. 

## Mise √† jour des requ√™tes üöÄ

Comme nous venons d‚Äôajouter une colonne, il est important de mettre √† jour le code associ√©. 

Pour nous simplifier la vie, nous pouvons alimenter la nouvelle colonne via des triggers sur la mise √† jour et l‚Äôinsertion de la table en attendant la modification des requ√™tes dans le code.

Il reste cependant compliqu√© de mettre √† jour toutes les requ√™tes afin d‚Äôutiliser la nouvelle colonne distribu√©e. Dans notre cas, nous avons qu‚Äôune seule table, mais dans le cas d‚Äôune vraie application cela peut devenir complexe. 

## Copie des donn√©es dans le cluster üé≠

CitusData, bien que cela soit une extension, n√©cessite d‚Äô√™tre initialis√© dans un premier temps sans les donn√©es. Les donn√©es doivent alors √™tre ins√©r√©es via un `pg_restore`, car tous les nodes ne contiennent pas les m√™mes donn√©es.  

Nous allons alors dumper les donn√©es : 

```bash
pg_dump -h  [host] -U [user] [database_name] > [backup_name]
```

```bash
pg_restore -U [user] [db_name] [db_backup]
```

Nous pouvions aussi le faire via le Write-Ahead Logging (WAL) comme l‚Äôexplique PostgreSQL dans [sa documentation](https://www.postgresql.org/docs/9.1/continuous-archiving.html).

## Utiliser la base de donn√©e Citus üçã

Lorsqu‚Äôon souhaite effectuer la migration vers Citus, il faut alors r√©duire le traffic sur l‚Äôancienne base de donn√©es. Pour cela, nous pouvons passer nos applications en maintenance. Puis il suffit alors de changer l‚Äôurl de connexion √† la base de donn√©es vers celle de Citus. 

Nous pouvons ensuite enlever le mode maintenance et v√©rifier le bon fonctionnement.

![Screenshot 2022-01-01 at 15.57.57.png](images/screen-workers.png)

# Conclusion üéá

Finalement, gr√¢ce √† cette migration nous b√©n√©ficions du scaling propos√© par Citus.   

Malgr√© que ce soit une extension de PostgreSQL, nous avons pu voir n√©anmoins la complexit√© de la mise en place de celui-ci. 

Comme nous avons pu le voir, nous avons √©t√© oblig√© de migrer les donn√©es vers une instance de PostgreSQL utilisant Citus. La migration sans downtime est difficile √† effectuer.  

De plus comme indiqu√© dans la documentation, Citus est utile dans dans peu de cas d‚Äôusage, nous pouvons alors nous questionner sur la pertinence de ce dernier du fait de la complexit√© √† migrer vers celui-ci. Nous pensons qu‚Äôil y a surement des solutions plus adapt√©es : √©tant capable de faire plus de choses, ayant une plus grosse communaut√© et une documentation plus fournie.